# Архитектура для сервиса аренды самокатов

Статус: Draft / v1
Team: Багдасарян Анри Аркадьевич, Лукацкий Илья Алексеевич, Туруллин Захар Дмитриевич, Шарапова Лияна Айратовна

---

## 1. Scope и Domain

**Scope**

- Сервис аренды самокатов:
  - создание оффера,
  - создание заказа (старт поездки),
  - завершение заказа (финиш поездки),
  - получение информации о заказе и текущей стоимости.
- Работаем на уровне **одного backend-сервиса**, одной основной БД и одного кеша.
- Внешние зависимости: `scooters`, `users`, `zones`, `configs`, `payments`.

**Domain**

- Приложение, через которое пользователь:
  - выбирает самокат и тариф,
  - получает оффер,
  - начинает поездку,
  - получение текущей информации по заказу (например, стоимость)
  - завершает поездку и оплачивает её.
- Бизнес-ограничения:
  - нужно учитывать долги пользователя и его “надёжность”;
  - офферы должны быть "свежими";
  - часть источников данных критична (scooters, payments), часть — допускают деградации (users, configs, zones).

**Коротко о выбранных технологиях**:
- База данных: PostgreSQL с партиционированием таблицы `orders` по дате создания.
- Кеш: in-memory LRU (готовая реализация), держим активные заказы и конфигурационные данные.
- Сервис: Python (FastAPI) со слоями API/домен/хранилища/клиенты.

---

## 2. Технические цели

### 2.1. Функциональные требования (FR)

1. Создание оффера (stateless):
   - POST `/offers` — синхронный расчёт цены по самокату/зоне/тарифу; stateless, кешируется на стороне клиента.
   - Результат: открытая часть оффера (PartnerOffer для UI) + `pricing_token` (подписанный blob для верификации).
   - `pricing_token` содержит минимум: `user_id`, `expires_at`, `tariff_version`, `pricing_algo_version`, `offer_hash` (SHA-256 от канонического JSON открытой части).
- Клиент не может подделать токен (подписан нашим секретом, например JWT). Идемпотентность для `/offers` не требуется — повторный вызов просто пересчитает цену и выдаст новый токен (лимитер нужен, но оставляем за рамками).
2. Создание заказа (старт поездки):
- POST `/orders` — **идемпотентно** (по Idempotency-Key) создаём заказ из оффера (при условии валидного `pricing_token`).
   - Сериализуем открытый оффер канонически, считаем `offer_hash`, декодируем `pricing_token`, проверяем:
     - `token.user_id == user_id`,
     - `now <= token.expires_at`,
     - `token.offer_hash == offer_hash`,
     - `tariff_version` / `pricing_algo_version` допустимы (из конфигов).
- Затем проверяем актуальные данные: `user_summary` (долги/репутация), состояние самоката (внешний сервис), `zone/config` (внешние сервисы).
   - Для новых/ненадёжных пользователей — "закручиваем гайки".
   - Цену и тариф берём из переданного оффера: после валидации считаем его актуальным и корректным для формирования заказа.
   - Фиксируем заказ в базе данных.
3. Завершение заказа:
   - POST `/orders/{id}/finish` — расчёт итоговой стоимости по фактическому времени/дистанции, закрытие заказа.
   - Инициируем платеж и сохраняем финальный статус оплаты в заказе.
   - Идемпотентность по Idempotency-Key.
4. Получение информации о заказе и текущей стоимости:
   - GET `/orders/{id}` — вернуть состояние заказа, текущую/финальную стоимость, статус оплаты.
5. Контроль свежести оффера:
   - Проверяем `pricing_token.expires_at`; просроченный токен отвергаем с предложением запросить новый оффер (можно фолбекать с новым оффером).
   - `pricing_token` подписан нами, поэтому нельзя поменять цену/идентификаторы без обнаружения.
6. Учёт долгов и “надёжности” пользователя:
   - В `user_summary` храним `current_debt`, `rides_count`, `last_payment_status`.
   - Блокируем старт при долге > 0; при низком `rides_count` или росте суммарных долгов по системе включаем усиленные проверки (обязательный pre-check платежей, ограничения тарифов/длины поездки).
7. Интеграция с внешними сервисами:
   - `scooters` — проверка доступности, финализация состояния после завершения.
- `users` — профиль, репутационный скор;
- `zone` — логика тарифных зон, read-only;
- `configs` — флаги TTL/лимитов;
   - `payments` — pre-check оплата/удержание/финальное списание.

---

### 2.2. Нефункциональные требования (NFR)

**Исходные параметры задачи**

- `X = 20` — RPS на создание заказов (старт поездки).
- `Y = 50` — среднее число показов информации о заказе на один заказ.
- `Z = 10 КБ` — средний размер записи о заказе в БД.
- Хранение данных — до 5 лет (влияет на расчёт ресурсов, не обязательно всё держать “горячим”).

1. **Производительность / нагрузка**
   - Throughput: ~`X = 20 RPS` создание заказов, ~`20 RPS` завершение, ~`1000 RPS` GET `/orders/{id}`, `/offers` до `40 RPS`.
   - Read load: Y = 50 просsмотров/заказ; нагрузка на БД — сотни read/сек, кеш должен снимать большую долю GET `/orders/{id}`.
2. **Хранение**
   - “Горячее” (оперативные) данные — ~30 дней в основной БД.
   - Общий горизонт хранения — 5 лет (архив/отдельное хранилище).
3. **Надёжность и доступность (Reliability)**
   - SLA/SLO: целимся в 99.9% для старт/финиш заказов.
   - Критичные зависимости: `scooters`, `payments` — при недоступности отказываем в обслуживание с сообщением попробовать чуть позже.
   - Некритичные: `users`, `configs`, `zones` — работаем через кеш/статический конфиг/последний профиль в ограниченном режиме.
4. **Масштабируемость (Scalability)**
   - Горизонтальное масштабирование API и кеш-слоя; партиционирование `orders`, stateless `/offers` снижает нагрузку на хранилища.
5. **Поддерживаемость (Maintainability)**
   - Чёткое разделение слоёв: transport -> handlers -> domain -> storage/clients -> caching.
   - Возможность выделить модули в отдельные сервисы позже без массового переписывания.
6. **Наблюдаемость (Observability)**
   - Метрики: латентности/успехи по ручкам, RPS, error rate, доля кеш-хитов GET `/orders/{id}`, время внешних вызовов, доля успешных платежей, конверсия offers->orders, отказы по скорингу; логирование расчётов офферов и платежных операций.

---

### 2.3. Оценки нагрузки (RPS, БД, кеш, диски)

#### 3.3.1. RPS по API (грубые оценки)

Берём параметры задачи:

- X = 20 — RPS на создание заказов.
- Y = 50 — просмотров заказа на один заказ.
- Считаем, что оффер формируем примерно в 2 раза чаще, чем реально стартуют поездки (конверсия 50%).

| API                        | Оценка RPS                         | Комментарий                                        |
|---------------------------|------------------------------------|----------------------------------------------------|
| POST `/offers`            | до **40 RPS**                      | не каждый оффер конвертируется в заказ (pure function, нужен лиммитер и кеш на клиенте)            |
| POST `/orders`            | **20 RPS**                         | из условия X                                      |
| POST `/orders/{id}/finish`| **20 RPS**                         | у каждого заказа один финиш                       |
| GET `/orders/{id}`        | **около 1000 RPS**                 | X * Y = 20 * 50                                   |

Итого фронтовая нагрузка:

- write-ручки: ~80 RPS (offers + create + finish),
- read-ручки: ~1000 RPS.

#### 3.3.2. Нагрузка на БД (Postgres)

Пусть:

- каждый заказ -> **1 INSERT** в `orders` при создании + **1 UPDATE** при завершении;
- при завершении заказа обновляем агрегаты в `user_summary` (`current_debt`, `rides_count` и т.п.) -> ещё **до 2 UPDATE**’ов;
- при создании заказа читаем:
  - `user_summary` (1 read).

Грубо (только `orders` + агрегаты, офферы stateless):

- **writes в БД:**
  - `orders`: 20 INSERT + 20 UPDATE ≈ 40 write-операций/сек;
  - `user_summary`: до 40 UPDATE/сек;
  - итого: порядка **80 write оп/сек**.

- **reads в БД:**
  - создание заказа: читаем `user_summary` (1) -> 1 read * 20 RPS ≈ **20 read/сек**;
  - завершение заказа: читаем заказ (обычно из кеша; 20% miss -> 0.2 read), `user_summary` (1) -> ~1.2 read * 20 RPS ≈ **24 read/сек**;
  - GET `/orders/{id}`: ~80% кеш-хиты, 20% miss -> 0.2 read * 1000 RPS = **200 read/сек**;
  - итого: ~**244 read/сек** реальной нагрузки на базу данных при указанных допущениях.

#### 3.3.3. Нагрузка на кеш

- **GET `/orders/{id}`**:
  - общий RPS ~1000;
  - если держать активные/недавно завершённые заказы в кеш-снимке, большая часть обращений уходит в кеш;
  - оценочно: ~800 RPS в кеш (hits) + 200 RPS в базу данных (misses).

- **Кеш зон/конфигов**:
  - `zones` — LRU + TTL 10 минут (read-only источник);
  - `configs` — статический локальный конфиг как fallback + периодический fetch из сервиса (до 1 раза в минуту);
  - нагрузка по RPS на кеш тут мизерная по сравнению с заказами.

#### 3.3.4. Объём данных (хранение)

По условию:

- X = 20 заказов/сек = 20 * 86400 = **1 728 000 заказов в день**;
- Z = 10 КБ на заказ -> **≈ 16,5 ГБ/день** сырой информации (1.7M * 10 KB);
- За месяц (30 дней): 16,5 * 30 ≈ **495 ГБ** ≈ 0.5 ТБ “горячих” данных в таблице `orders`.

За год:

- 1 728 000 * 365 ≈ **630 млн заказов**;
- 630 млн * 10 КБ ≈ **~5.8 ТБ** сырых данных/год.

За 5 лет:

- ~29 ТБ сырых данных;
- с учётом индексов, возможных реплик и бэкапов — **с запасом считаем ~до 90 ТБ** в архиве.

План по хранению:

- **“Горячее” данные**: последние 30 дней (~0.5 ТБ) храним в основной БД (Postgres, партиции по дню/неделе).
- Данные старше 30 дней уезжают во внешнее архивное хранилище (вторая БД с HDD / S3) — обсуждается отдельно, но логика на основную БД не завязана.
- Офферы не храним; при необходимости для аудита/аналитики можно логировать расчёты в отдельное хранилище.

#### 3.3.5. Нагрузка на диск, сеть, CPU (оценки)

- **Диск (PostgreSQL)**: ~40 write оп/сек и ~270 read/сек по текущим оценкам; объём горячих данных ~0.5 ТБ/мес. Потоки: ~400 КБ/с на записи и ~2.7 МБ/с на чтения (по 10 КБ на запись/чтение). Требуются SSD, wal_compression, партиционирование.
- **Сеть (внешние сервисы)**: на старт/финиш ~2–3 исходящих вызова (scooters, payments, users/configs/zones) * ~40 RPS суммарно -> до ~120 внешних RPS; при 1 КБ запрос/ответ это ~240 КБ/с. GET `/orders/{id}` наружу не ходит. Внутри — обращения к локальному кешу (in-memory, на том же хосте, без сетевой стоимости) и к БД (~3.1 МБ/с суммарно read+write).
- **CPU**: основное — JSON-сериализация/валидация, проверка `pricing_token` (JWT), расчёт цены, валидация входных данных. Эти части CPU-bound, но лёгкие; сетевые вызовы и работа с БД/диском — IO-bound. Пиковая нагрузка — на пути старт/финиш с внешними вызовами.

---

## 4. Архитектура

### 4.1. Компоненты

- **HTTP API сервис**:
  - реализует все ручки `/offers`, `/orders`, `/orders/{id}`, `/orders/{id}/finish`;
  - внутри разделён на доменные модули (offers, orders, pricing, users, scooters, payments).
  - Логирование: входные/выходные события по заказам и платежам, ошибки внешних вызовов, бизнес-события (start/finish/payment status).
  - Метрики (базовые): latency/успехи по ручкам, RPS, error rate, доля кеш-хитов GET `/orders/{id}`, время ответа внешних сервисов, доля успешных платежей, конверсия offers->orders, долги/отказы по скорингу.

- **PostgreSQL**:
  - основная транзакционная БД (храним только `OrderData` + минимум агрегатов):
    - таблица `orders` (partitioned by date);
    - таблица `user_summary`;

- **Кеш-слой (in-memory, локальный на узле)**:
  - кеш заказов для GET `/orders/{id}`,
  - кеш тарифных зон,
  - кеш конфигов,
  - хранилище для Idempotency-ключей,

- **Внешние сервисы**:
  - `zones` — read-only, отдаёт тариф в зоне;
  - `configs` — динамические конфиги (read-only), поверх локального статического fallback;
  - `scooters` — чтение состояния + write-операции на бронь/разблокировку;
  - `users` — чтение профиля + запись итогов поездки;
  - `payments` — pre-check на старте и финальное списание на финише.

### 4.2. Диаграмма архитектуры

TODO

---

### 4.3. Архитектурное влияние на требования:

1. **Создание / завершение заказа (FR 2 и 3)**  
   - Transactional write в Postgres (`orders` + update `user_summary`).  
   - Идемпотентность за счёт Idempotency-Key:
     - уникальный ключ в таблице `orders`,
     - плюс запись результата в кеш на время.

2. **GET `/orders/{id}` (FR 4)**  
   - Основная нагрузка снимается через кеш (ограниченного размера, хранить имеет смысл только за последние 3-4 часа (конфигурируемо) - кешируем сериализованный заказ на время активной поездки + немного после завершения).
   - БД используется для miss’ов, а также для выборок, которые не попали в кеш.

3. **Свежесть оффера (FR 5)**  
   - Оффер stateless; свежесть контролируется `expires_at`.  
   - При создании заказа сверяем `now() <= expires_at` и `offer_hash` из токена, иначе ошибка и просьба пересчитать оффер.

4. **Долги и репутация (FR 6)**  
   - В таблице `user_summary` всегда есть текущий `current_debt` и `rides_count`.  
   - Логика создания заказа сначала проверяет эти поля:
     - при долге > 0 -> блокировка;
     - при небольшом `rides_count` и высоких долгах — дополнительные проверки или отказ.

5. **Хранение и архивация (NFR 4)**  
   - `orders` партиционируется по дате создания.  
   - Раз в день старые партиции (старше 30 дней) копируются в внешнее хранилище и удаляются из горячей БД.  
   - `user_summary` — “горячая” таблица; зоны/конфиги храним в кеше или локальном файле, `scooter_state` — внешний сервис.

6. **Доступность и деградация (NFR 5)**  
   - На уровне кода: таймауты и fallback-логика для внешних сервисов.  
   - Продуманные режимы “без users”, “без configs” и т.п. (ниже будет отдельный раздел про деградацию): зоны/конфиги читаем из кеша/статического файла, users — возвращаем последний кешированный профиль, payments — быстро отказываем без зависаний.

7. **Maintainability (NFR 6)**  
   - Сервис структурируется слоями:
     - `api` (FastAPI endpoints),
     - `services` (доменные use-cases),
     - `repositories` (Postgres),
     - `clients` (HTTP-клиенты к `scooters`, `users`, `payments`, `zones`, `configs`),
     - `caching` (кеш-слой).  
   - Это задаёт явные границы, которые можно потом вынести в отдельные сервисы.

---

### 4.4. Развёртывание и миграции

- Docker + docker-compose: поднимаем сервис, Postgres, кеш.
- Миграции БД через shell/sql скрипты.
- Конфиги через env/файлы.

---

## 5. Деградация по зависимостям

- **PostgreSQL (orders) — критично**  
  - Нет записи в `orders` -> блокируем создание и принудительное завершение заказов с быстрым отказом 503; GET отдаём из кеша до истечения TTL, при miss сразу 503.

- **Cache**  
  - При отказе кеша все GET `/orders/{id}` идут в БД -> рост латентности, снижаем лимиты на запросы, наблюдаем за БД (чтобы избежать амплификации).  
  - Кеш зон/конфигов недоступен -> больше обращений к `configs`/`zones`, включаем консервативные таймауты.

- **scooters — критично**  
  - Старты/финиши требуют проверки/брони состояния самоката. При недоступности: запрещаем создание и принудительное завершение заказов с чёткой ошибкой; Read-запросы продолжают работать.

- **payments — критично**  
  - Старты для рискованных/новых пользователей запрещаем; финиш без доступа к платежам -> ставим заказ в `payment_pending`, возвращаем ошибку клиенту.  
  - Возможен глобальный стоп на новые поездки (через конфиги) при длительной недоступности.

- **users — некритично (фоллбек)**  
  - Если сервис недоступен, используем последний кешированный профиль; при отсутствии данных — консервативная политика: pre-check, базовый жадный тариф.

- **configs — некритично (фоллбек)**  
  - При недоступности используем кеш; новые значения не подтягиваются, работают значения по умолчанию (статический конфиг).

- **zones — некритично (read-only)**  
  - При недоступности отдаём из кеша; если нет кеша по зоне — можем применить базовый жадный тариф с консервативной ценой.

---

## 6. Альтернативы

- **Stateful `/offers`**  
  Держать офферы в БД/кеше, сделать `/offers` идемпотентным, хранить `offer_id` и читать его в `/orders`.  
  Плюсы: проще валидация на `/orders`, аудит рядом, можно переиспользовать `offer_id`.  
  Минусы: короткоживущие write-нагрузки, инвалидация/TTL (нужно думать как чистить протухшее), сложнее деградация.
  При необходимости добавления аналитики или аудита по офферам в нашей схеме: можно логировать расчёты офферов и отправлять отдельный сервис/хранилище (замечание на будущее).

- **Кеш-слой**  
  Альтернатива in-memory LRU — внешнее кеш-хранилище (Redis/KeyDB/Memcached) для лучшей горизонтальной масштабируемости; минус — дополнительная инфраструктура и сетевые RTT.

- **Хранение зон/конфигов**  
  Вместо чистого кеша можно дублировать зоны/конфиги в локальной или дублирующей БД (реплика или периодический импорт) для лучшей автономности; минус — сложнее синхронизация и потенциальные рассинхроны версий.

- **БД: SQLite вместо PostgreSQL**  
  Плюсы: простота развёртывания, отсутствие отдельного сервера, быстрее сетевых походов.  
  Минусы: ограниченная конкурентность и масштабирование, сложнее партиционирование/реплики, риски на высоких нагрузках; не соответствует целевым RPS/объёмам.

- **БД: NoSQL (MongoDB/Cassandra) вместо реляционной**  
  Плюсы: горизонтальная масштабируемость, гибкая схема.  
  Минусы: сложнее транзакционная консистентность для заказов/платежей, придётся самим реализовывать идемпотентность/ограничения, нет привычных SQL-инструментов; для транзакционной модели заказов реляционка уместнее; важно также, что PostgreSQL имеет лучшую документацию и поддержку, и больше всего знакома команде.
